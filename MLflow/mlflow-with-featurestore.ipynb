{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36b8b8fd-59b2-467c-bc89-c0e451172fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### MLflow introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "540face9-0e70-4f15-8f25-b02f4c7d6984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 1. Importing the desired libraries and creating training set from the registered feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068a9c2a-56e1-46b0-9375-29bb3c266986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "from databricks.feature_store import FeatureLookup\n",
    "import typing\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f3092a-2286-46c8-84c5-a6ad40d80138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Name of the model\n",
    "MODEL_NAME = \"random_forest_classifier_featurestore\"\n",
    "#This is the name for the entry in model registry\n",
    "MODEL_REGISTRY_NAME = \"Bank_Customer_Churn\"\n",
    "#The email you use to authenticate in the Databricks workspace\n",
    "USER_EMAIL = \"samuel.hmariam@shewit.co.uk\"\n",
    "#Location where the MLflow experiement will be listed in user workspace\n",
    "EXPERIMENT_NAME = f\"/Users/{USER_EMAIL}/Bank_Customer_Churn_Analysis\"\n",
    "# we have all the features backed into a Delta table so we will read directly\n",
    "FEATURE_TABLE = \"bank_churn_analysis.bank_customer_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ace7d85-75f7-4f82-a365-6666a40302da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Feature_Lookup_Input_Tuple(typing.NamedTuple):\n",
    "  fature_table_name: str\n",
    "  feature_list: typing.Union[typing.List[str], None] \n",
    "  lookup_key: typing.List[str]\n",
    "\n",
    "# this code is going to generate feature look up based on on the list of feature mappings provided.\n",
    "def generate_feature_lookup(feature_mapping: typing.List[Feature_Lookup_Input_Tuple]) -> typing.List[FeatureLookup]:  \n",
    "  lookups = []\n",
    "  for fature_table_name, feature_list, lookup_key in feature_mapping:\n",
    "    lookups.append(\n",
    "          FeatureLookup(\n",
    "          table_name = fature_table_name,\n",
    "          feature_names = feature_list,\n",
    "          lookup_key = lookup_key \n",
    "      )\n",
    "    )\n",
    "  return lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2835469b-a69c-4488-9cb5-ad932f464805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 2. Build a simplistic model that uses the feature store table as its source for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a12357-9164-495d-86ef-4a3249163bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#initialize the feature store client\n",
    "fs = FeatureStoreClient()\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2dd1e9-f910-4024-8f7b-244d7e80ab91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():  \n",
    "  TEST_SIZE = 0.20\n",
    "  \n",
    "  #define the list of features we want to get from feature table\n",
    "  #If we have to combine data from multiple feature tables then we can provide multiple mappings for feature tables \n",
    "  features = [Feature_Lookup_Input_Tuple(FEATURE_TABLE,[\"CreditScore\" , \"Age\", \"Tenure\",\\\n",
    "              \"Balance\", \"NumOfProducts\", \"HasCrCard\",\\\n",
    "              \"IsActiveMember\", \"EstimatedSalary\", \"Geography_Germany\",\\\n",
    "              \"Geography_Spain\", \"Gender_Male\", \"Is_Balance_Zero\", \"Tenure_to_Age\", \"Balance_to_Salary\"], [\"CustomerId\"] )]\n",
    "\n",
    "  lookups = generate_feature_lookup(features)\n",
    "  \n",
    "  #Now we will simulate receiving only ID's of customers and the label as input at the  time of inference\n",
    "  training_df = spark.table(FEATURE_TABLE).select(\"CustomerId\", \"Exited\")\n",
    "  \n",
    "  #Using the training set we will combine the training dataframe with the features stored in the feature tables.\n",
    "  training_data = fs.create_training_set(\n",
    "    df=training_df,\n",
    "    feature_lookups=lookups,\n",
    "    label=\"Exited\",\n",
    "    exclude_columns=['CustomerId']\n",
    "  )\n",
    "  \n",
    "  #convert the dataset to pandas so that we can fit sklearn RandomForestClassifier on it\n",
    "  train_df = training_data.load_df().toPandas()\n",
    "  \n",
    "  #The train_df represents the input dataframe that has all the feature columns along with the new raw input in the form of training_df.\n",
    "  X = train_df.drop(['Exited'], axis=1)\n",
    "  y = train_df['Exited']\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=54, stratify=y)\n",
    "  \n",
    "  #here we will are not doing any hyperparameter tuning however, in future we will see how to perform hyperparameter tuning in scalable manner on Databricks.\n",
    "  model = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "  signature = mlflow.models.signature.infer_signature(X_train, model.predict(X_train))\n",
    "  \n",
    "  predictions = model.predict(X_test)\n",
    "  fpr, tpr, _ = metrics.roc_curve(y_test, predictions, pos_label=1)\n",
    "  auc = metrics.auc(fpr, tpr)\n",
    "  accuracy = metrics.accuracy_score(y_test, predictions)\n",
    " \n",
    "  #get the calculated feature importances.\n",
    "  importances = dict(zip(model.feature_names_in_, model.feature_importances_))  \n",
    "  #log artifact\n",
    "  mlflow.log_dict(importances, \"feature_importances.json\")\n",
    "  #log metrics\n",
    "  mlflow.log_metric(\"auc\", auc)\n",
    "  mlflow.log_metric(\"accuracy\", accuracy)\n",
    "  #log parameters\n",
    "  mlflow.log_param(\"split_size\", TEST_SIZE)\n",
    "  mlflow.log_params(model.get_params())\n",
    "  #set tag\n",
    "  mlflow.set_tag(MODEL_NAME, \"mlflow and feature store demo\")\n",
    "  #log the model itself in mlflow tracking server\n",
    "  mlflow.sklearn.log_model(model, MODEL_NAME, signature=signature, input_example=X_train.iloc[:4, :])\n",
    "\n",
    "  # finally to make the feature store track what features are being used by our model we call log_model with the feature store client\n",
    "  fs.log_model(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    flavor=mlflow.sklearn,\n",
    "    training_set=training_data,\n",
    "    registered_model_name=MODEL_REGISTRY_NAME\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033b21b3-63a5-4862-b667-dddee80d43cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 3. Now that we have the model logged to the MLflow tracking server, we can get the latest version from the experiment and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc4dd41-d53f-47a8-a6dc-4b8889d7bd43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "#initialize the mlflow client\n",
    "client = MlflowClient()\n",
    "#get the experiment id \n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "#get the latest run id which will allow us to directly access the metrics, and attributes and all th einfo\n",
    "run_id = mlflow.search_runs(experiment_id, order_by=[\"start_time DESC\"]).head(1)[\"run_id\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7dfbc74-fe9e-4847-bfaf-b574af00c93b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#at the time of infernce you can provide just the CustomerId. This is the key that will perform all the lookup for the features automatically.\n",
    "predictions = fs.score_batch(f\"runs:/{run_id}/{MODEL_NAME}\", spark.table(FEATURE_TABLE).select(\"CustomerId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff4e615-dc00-4a57-ba98-6f78337e66d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4935a00-72d4-4300-9cc1-4245b2f80e19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "262e8550-0aca-4c98-be8d-a44e6dcaf992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "#get all the information about the current experiment\n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "\n",
    "#list all the runs that are part of this experiment and delete them\n",
    "runs = mlflow.list_run_infos(experiment_id=experiment_id)\n",
    "for run in runs:\n",
    "  mlflow.delete_run(run_id = run.run_id)\n",
    "\n",
    "#finally delete the experiment  \n",
    "mlflow.delete_experiment(experiment_id=experiment_id)  \n",
    "\n",
    "client = MlflowClient()\n",
    "#delete the model registered in the registry to clear the linkage in thefeature store\n",
    "client.delete_registered_model(name=MODEL_REGISTRY_NAME)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mlflow-with-featurestore",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
