{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56dc45a8-6e46-4770-a55c-af8244f0cf3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Step1: Ingest Data to Notebook\n",
    "\n",
    "We will download the dataset hosted at  [**Kaggle**](https://www.kaggle.com/mathchi/churn-for-bank-customers)\n",
    "\n",
    "#### Content\n",
    "*   `RowNumber` —corresponds to the record (row) number and has no effect on the output.\n",
    "*   `CustomerId` -contains random values and has no effect on customer leaving the bank.\n",
    "*   `Surname` —the surname of a customer has no impact on their decision to leave the bank.\n",
    "*   `CreditScore` —can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n",
    "*   `Geography` —a customer’s location can affect their decision to leave the bank.\n",
    "*   `Gender` —it’s interesting to explore whether gender plays a role in a customer leaving the bank\n",
    "*   `Age` —this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n",
    "*   `Tenure` —refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank\n",
    "*   `Balance` —also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n",
    "*   `NumOfProducts` —refers to the number of products that a customer has purchased through the bank.\n",
    "*   `HasCrCard` —denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n",
    "*   `IsActiveMember` —active customers are less likely to leave the bank\n",
    "*   `EstimatedSalary` —as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n",
    "*   `Exited` —whether or not the customer left the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f92fda5-bfe3-4164-a62c-8742976f3ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8935ff90-7112-420f-b1e5-8ca88e38cc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "bank_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"file:{os.path.dirname(os.getcwd())}/data/churn.csv\")\n",
    "display(bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2867a15-7de4-408f-bbf8-1d281ece00e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets get unique value count in Surname\n",
    "bank_df.select('Surname').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "358bc411-f3a0-4259-a6ea-7a46c602e138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop RowNumber and Surname as they add no value to our model\n",
    "bank_df = bank_df.drop('RowNumber', 'Surname')\n",
    "display(bank_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06ec5ec-b249-4f4f-821b-db846202399e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Feature Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b08fdb3-f3d6-40f5-a53c-e760efede499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Defining a database to store feature tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ccc03b-4031-4890-8a04-00c1a9277772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog() AS current_catalog, current_schema() AS current_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90684bc0-2116-4437-8182-406ae1ad63df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABASE_NAME = \"bank_churn_analysis\"\n",
    "#setup database that will hold our Feature tables in Delta format.\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1a426b-858b-470c-ada9-ec8535d18f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bank_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{DATABASE_NAME}.raw_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eff5d5c-1917-4aae-931c-71789e46e475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Defining a feature engineering function that will return a Spark dataframe with a unique primary key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64437bd0-181f-4bed-8941-8ce29b9a1078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "def compute_features(spark_df):\n",
    "    # https://spark.apache.org/docs/latest/api/python/migration_guide/koalas_to_pyspark.html?highlight=dataframe%20pandas_api\n",
    "    # Convert to pyspark.pandas DataFrame\n",
    "    ps_df = spark_df.pandas_api()\n",
    "    \n",
    "    # One-Hot Encoding for Geography and Gender\n",
    "    ohe_ps_df = ps.get_dummies(\n",
    "      ps_df, \n",
    "      columns=[\"Geography\", \"Gender\"],\n",
    "      dtype=\"int\",\n",
    "      drop_first=True\n",
    "    )\n",
    "    \n",
    "    # Clean up column names\n",
    "    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r' ', '', regex=True)\n",
    "    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r'\\(', '-', regex=True)\n",
    "    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r'\\)', '', regex=True)\n",
    "    \n",
    "    ## Additional example feature engineering steps\n",
    "\n",
    "    # Create a binary feature indicating whether the balance is zero or not\n",
    "    ohe_ps_df['Is_Balance_Zero'] = (ohe_ps_df['Balance'] == 0).astype('int')\n",
    "    \n",
    "    # Ratio of Tenure to Age\n",
    "    ohe_ps_df['Tenure_to_Age'] = ohe_ps_df['Tenure'] / ohe_ps_df['Age']\n",
    "    \n",
    "    # Interaction feature: Balance to EstimatedSalary ratio\n",
    "    ohe_ps_df['Balance_to_Salary'] = ohe_ps_df['Balance'] / ohe_ps_df['EstimatedSalary']\n",
    "    \n",
    "    return ohe_ps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc256098-1a76-4c95-be19-eae05e3f97d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable ANSI mode\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "\n",
    "# Compute features\n",
    "bank_features_df = compute_features(bank_df)\n",
    "display(bank_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85320a8b-c954-4274-9d6f-5ce78fdc14ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Create the Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce25fed3-4199-4b39-9be3-5e21f17c5e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the feature store client using `FeatureStoreClient()`.\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "fs = FeatureStoreClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4fff33-5bc2-43c9-bb0c-7f5cd02fe5a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bank_feature_table = fs.create_table(\n",
    "  name=f\"{DATABASE_NAME}.bank_customer_features\", # the name of the feature table\n",
    "  primary_keys=[\"CustomerId\"], # primary key that will be used to perform joins\n",
    "  schema=bank_features_df.spark.schema(), # the schema of the Feature table\n",
    "  description=\"This customer level table contains one-hot encoded categorical and scaled numeric features to predict bank customer churn.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e47636-fc8e-40a5-a2d5-51f0f751391d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. Populate the feature table using write_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571bae8b-c7ff-4b03-9ac1-fb5a8d8b643c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs.write_table(df=bank_features_df.to_spark(), name=f\"{DATABASE_NAME}.bank_customer_features\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c9f0170-60ef-4f7f-bce9-da3b702a202d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f625649-7db7-46f0-8423-a04a3d006fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " fs.drop_table(\n",
    "   name=f\"{DATABASE_NAME}.bank_customer_features\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8573699131239408,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "churn-analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
